This code is designed to address the challenge of few-shot semantic segmentation in dual-source remote sensing imagery. Compared to conventional prototype networks, our method extends prototype learning to handle dual-modal data. During episodic task construction, we adopt a feature-driven sampling strategy: the support set is composed of samples with high intra-class dispersion to enhance generalization, while the query set includes both randomly selected and hard samples that resemble prototypes of other classes. We then apply superpixel pooling based on a shared superpixel segmentation, ensuring that the PAN and MS modalities produce superpixels with consistent quantity and semantic alignment. Cross-modal similarity is computed between superpixel features, and the top 30% most important regions from each modality are selected; their intersection defines the key regions used for alignment. For contrastive alignment, we take query samples from one modality as anchors and construct positive samples from the other modality using the same-class prototype, the most similar, and the most dissimilar same-class support samples. Negative samples include the most similar different-class prototype and the top two hardest different-class support samples. A margin-based contrastive loss is computed on these relations. Finally, the cross-modal similarity matrix is used as an attention weight to guide feature fusion in the key regions. The fused features are then used for classification, with a cross-entropy loss computed on the query predictions. The total training objective combines both the contrastive and classification losses.
![总流程新](https://github.com/user-attachments/assets/d2cce177-7a0f-4fc9-93f6-5034b3fddb68)
